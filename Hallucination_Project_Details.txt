Project Title: Hallucination Detection & Mitigation for LLMs when Citing References

Date: September 17, 2025

Project Constraint: Medium Complexity, 6-Month Duration, Team of 2 Members, RTX 3070Ti GPU (can be upgraded to 3090Ti or A100 if needed)

## 1. Project Content Focus

This project focuses on detecting and mitigating factual hallucinations in Large Language Model (LLM) outputs, with a specific emphasis on scenarios where the model needs to cite references to support its claims. The core goal is to develop a lightweight, plug-and-play verifier module that can assess the factual accuracy of a statement against a reliable knowledge source and reduce the generation of incorrect or "hallucinated" information without relying on a massive "judge" model.

---

## 2. Project Details from Roadmap

The following is the established plan from the `FYP_Selected_Projects_Roadmap.txt` document.

- **Brief Introduction:** Detect factual hallucinations in LLM outputs and reduce them using a lightweight, plug-and-play verifier module without needing a huge "judge" model.
- **Technical Innovation:** A lightweight verifier ensemble combining retrieval evidence scoring, contradiction detection (NLI), and model uncertainty signals (e.g., self-consistency, entropy).
- **Technical Stack:** Python, Transformers, PEFT (LoRA/QLoRA), Vector DBs (FAISS/Chroma), NLI models (e.g., DeBERTa).
- **Complexity:** Medium.

**6-Month Compact Roadmap & Workflow:**
- **Month 1: Research & Planning**
  - **Literature Review:** Deep dive into papers on hallucination, fact-checking, RAG, and self-verification (e.g., Self-RAG, CoVe).
  - **Data Sourcing:** Download and prepare all required datasets: Wikipedia corpus, FEVER, NLI datasets (SNLI/MNLI), and hallucination benchmarks (TruthfulQA, RAGTruth).
  - **System Architecture Design:** Finalize the design for the generator-retriever-verifier pipeline, defining the inputs and outputs for each module.
  - **Setup Development Environment:** Configure Python environment, install necessary libraries (Transformers, PEFT, FAISS), and set up GPU access.

- **Month 2: Baseline & Retrieval Module**
  - **Data Preparation:** Preprocess and chunk the Wikipedia corpus. Generate embeddings and index them into a FAISS vector database.
  - **Retriever Implementation:** Build and optimize the hybrid retrieval module, combining BM25 (sparse) and a dense retriever. Evaluate its performance on a benchmark like MS MARCO or a BEIR subset.
  - **Baseline RAG Implementation:** Implement a baseline RAG system by integrating a generator LLM with the retriever.

- **Month 3: Verifier Module - Part 1**
  - **Uncertainty Signal Detection:** Implement modules to calculate uncertainty from the generator's output, focusing on token-level entropy and self-consistency scores.
  - **Evidence Alignment Scorer:** Train a simple classification model on claim-evidence pairs (from FEVER or synthetic data) to score how well evidence supports a claim.

- **Month 4: Verifier Module - Part 2**
  - **NLI Contradiction Detector:** Fine-tune a small NLI model (e.g., DeBERTa) to act as a contradiction detector between the LLM's claim and the retrieved evidence.
  - **Ensemble Verifier Integration:** Combine all signals (uncertainty, alignment score, NLI result) into a single, powerful ensemble verifier, possibly using a weighted heuristic or a meta-classifier.

- **Month 5: Mitigation & Evaluation**
  - **Mitigation Strategy Implementation:** Implement control logic based on the verifier's score to trigger actions like re-generation, adding warnings, or citing sources.
  - **End-to-End Evaluation:** Evaluate the complete system's ability to reduce hallucinations on benchmarks (TruthfulQA, RAGTruth) and the custom-built citation evaluation set.
  - **Performance Analysis:** Measure key metrics like factual consistency, hallucination rate, and attribution precision.

- **Month 6: Finalization & Documentation**
  - **Ablation Study:** Analyze the contribution of each verifier component by systematically disabling them and measuring the impact on performance.
  - **Final Report & Demo:** Write the final project report, detailing the architecture, methodology, results, and insights. Prepare a compelling demo of the system.
  - **Code Cleanup & Handoff:** Clean, comment, and document the codebase for future use and handoff.

---

## 3. Data Requirements and Collection

### Data Needs:

1.  **Evidence Corpus (for Retrieval and Citation):**
    *   **English Wikipedia:** The preferred source. It should be chunked at the sentence or small-window level, retaining metadata like `page_id` and paragraph/sentence spans for precise source attribution.

2.  **Supervised Training Signals:**
    *   **Claim-Evidence-Label Triples:** Datasets like **FEVER/FEVEROUS** provide "Supported," "Refuted," and "Not Enough Info" labels with corresponding evidence sentences. **KILT-FEVER** adds traceable page-level provenance.
    *   **General-Purpose NLI Datasets:** **SNLI, MNLI, and ANLI** are crucial for training the model to better understand entailment and contradiction.
    *   **RAG-Specific Hallucination Data:** **RAGTruth** offers a benchmark with RAG-generated answers and hallucination labels, which is highly relevant.

3.  **Retrieval/Reranking Datasets:**
    *   Subsets from the **BEIR** benchmark (e.g., **NQ, HotpotQA**) can be used to train and evaluate the retriever.
    *   **MS MARCO Passage Ranking** dataset is an optional but valuable resource for training a baseline reranker model.

4.  **Testing & Specialized Evaluation Sets:**
    *   A subset of **TruthfulQA** and the **RAGTruth** test set will be used for standardized evaluation.
    *   A **small-scale, custom-built "citation-annotated" evaluation set** (approx. 300â€“500 items) will be created. In this set, each atomic claim within a model's response will be labeled, and its cited evidence will be marked as "Supported," "Contradictory," or "Insufficient," with the evidence span precisely located.

### Data Collection Methods:
1.  **Knowledge Corpus:** Download and process a recent dump of the English Wikipedia. Clean the text and index it into a vector database (e.g., FAISS) with sentence/chunk-level embeddings and metadata.
2.  **Public Datasets:** Download FEVER, KILT, SNLI/MNLI, RAGTruth, and BEIR subsets from official sources like Hugging Face Datasets or their respective GitHub repositories.
3.  **Custom Evaluation Set:** Manually or semi-automatically generate claims from a source (e.g., Wikipedia) and have human annotators label the claims and the quality of their citations against the source documents.

---

## 4. Supporting Research

The project builds upon established and recent research in the following areas:

### Overviews & Surveys
- **Paper:** "A Survey on Hallucination in Large Language Models" (Zhang et al., 2023)
- **Use:** Provides a comprehensive overview of the types of hallucinations, their causes, and the state-of-the-art detection and mitigation techniques, which helps to frame the project's contribution.
- **Link:** https://arxiv.org/abs/2311.05232

### Foundational Fact-Checking & Attribution
- **Paper:** "FEVER: a Large-scale Dataset for Fact Extraction and VERification" (Thorne et al., 2018)
- **Use:** Pioneered the claim-evidence-label framework ("Supported/Refuted/NotEnoughInfo"), establishing the methodology for claim-by-claim verification.
- **Link:** https://arxiv.org/abs/1803.05355

- **Paper:** "KILT: a Benchmark for Knowledge Intensive Language Tasks" (Petroni et al., 2021)
- **Use:** Created a benchmark for knowledge-intensive tasks with source attribution, emphasizing traceability to the page or paragraph level.
- **Link:** https://arxiv.org/abs/2009.02252

### Automated Factuality Evaluation
- **Paper:** "QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization" (Fabbri et al., 2021)
- **Use:** Provides a question-answering based method to evaluate factual consistency, which can be adapted to verify claims against evidence by generating questions from the claim and checking answers from the source document.
- **Link:** https://arxiv.org/abs/2112.08542

- **Paper:** "Evaluating the Factual Consistency of Abstractive Text Summarization" (Kryscinski et al., 2019)
- **Use:** Introduces a model to classify claim-evidence pairs as factually consistent or inconsistent, providing a direct blueprint for a component of our verifier module.
- **Link:** https://arxiv.org/abs/1910.12840

- **Paper:** "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization" (Laban et al., 2022)
- **Use:** Supports the project's core idea of using a Natural Language Inference (NLI) model for fine-grained, sentence-level alignment between the claim and the evidence to detect contradictions.
- **Link:** https://arxiv.org/abs/2111.09525

### Hallucination Benchmarking & Analysis
- **Paper:** "TruthfulQA: Measuring How Models Mimic Human Falsehoods" (Lin et al., 2021)
- **Use:** Demonstrated that models often generate plausible but false answers, highlighting the need for uncertainty estimation and verification.
- **Link:** https://arxiv.org/abs/2109.07958

- **Paper:** "RAGTruth: A Hallucination Benchmark for Retrieval-Augmented Generation" (2023)
- **Use:** A benchmark focused on hallucinations in RAG systems, showing that "cited hallucinations" are common even with retrieved evidence, which necessitates finer-grained evidence alignment and discrimination.
- **Link:** https://arxiv.org/abs/2401.00396

### Hallucination Detection & Mitigation Techniques
- **Paper:** "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models" (Manakul et al., 2023)
- **Use:** Uses self-consistency sampling to detect unreliable statements. This can serve as a useful uncertainty signal but is insufficient on its own to guarantee citation correctness.
- **Link:** https://arxiv.org/abs/2303.08896

- **Paper:** "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection" (Asai et al., 2023)
- **Use:** A model that reflects on its own generations and retrieves evidence for self-verification during the generation process, improving attribution and reducing hallucinations.
- **Link:** https://arxiv.org/abs/2310.11511

- **Paper:** "Chain-of-Verification Reduces Hallucination in Large Language Models" (Gao et al., 2023/2024)
- **Use:** An "answer-then-verify" process that automatically checks and rewrites its initial response, significantly improving the factuality of long-form answers.
- **Link:** https://arxiv.org/abs/2309.11495

### Critiques of LLM-as-a-Judge
- **Paper:** "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena" (Zheng et al., 2023)
- **Use:** Highlights the challenges and biases (e.g., position, verbosity, and self-preference biases) of using LLMs as evaluators. This supports the project's decision to use a more robust, multi-component verification system instead of relying solely on an LLM judge.
- **Link:** https://arxiv.org/abs/2306.05685

**Key Insight:** Research also indicates that using an LLM-as-a-judge can be prone to bias and instability, making it risky to rely solely on it for verifying citations. A more robust approach, which this project adopts, is to combine evidence retrieval with NLI-based contradiction detection.
