Project Title: Hallucination Detection & Mitigation for LLMs when Citing References

Date: September 17, 2025

Project Constraint: Medium Complexity, 6-Month Duration, Team of 2 Members, RTX 3070Ti GPU (can be upgraded to 3090Ti or A100 if needed)

## 1. Project Content Focus

This project focuses on detecting and mitigating factual hallucinations in Large Language Model (LLM) outputs, with a specific emphasis on scenarios where the model needs to cite references to support its claims. The core goal is to develop a lightweight, plug-and-play verifier module that can assess the factual accuracy of a statement against a reliable knowledge source and reduce the generation of incorrect or "hallucinated" information without relying on a massive "judge" model.

---

## 2. Project Details from Roadmap

The following is the established plan from the `FYP_Selected_Projects_Roadmap.txt` document.

- **Brief Introduction:** Detect factual hallucinations in LLM outputs and reduce them using a lightweight, plug-and-play verifier module without needing a huge "judge" model.
- **Technical Innovation:** A lightweight, **trainless** verifier ensemble combining multiple zero-shot signals. This includes:
  - **Intrinsic Uncertainty:** Using decoder statistics like token entropy.
  - **Self-Agreement Methods:** Analyzing response variability across multiple generations (Self-Consistency).
  - **Retrieval-Grounded Heuristics:** Calculating evidence coverage and lexical overlap.
  - **Zero-Shot NLI:** Using off-the-shelf models to check for entailment and contradiction between claims and evidence.
- **Technical Stack:** Python, Transformers, Vector DBs (FAISS/Chroma), and specific zero-shot NLI models (e.g., `MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli`).
- **Complexity:** Medium.

**6-Month Compact Roadmap & Workflow (Detector-Focused):**
- **Month 1: Research & Planning**
  - **Literature Review:** Deep dive into papers on **trainless** hallucination detection, fact-checking, RAG, and self-verification (e.g., SelfCheckGPT, CoVe).
  - **Data Sourcing:** Download and prepare all required datasets: Wikipedia corpus, FEVER, NLI datasets, and hallucination benchmarks (TruthfulQA, RAGTruth).
  - **System Architecture Design:** Finalize the design for the generator-retriever-verifier pipeline, focusing on a modular, trainless verifier.
  - **Setup Development Environment:** Configure Python environment, install necessary libraries (Transformers, FAISS), and set up GPU access.

- **Month 2: Baseline & Retrieval Module**
  - **Data Preparation:** Preprocess and chunk the Wikipedia corpus. Generate embeddings and index them into a FAISS vector database.
  - **Retriever Implementation:** Build a dense retriever. A hybrid approach with BM25 or a reranker can be added later if needed.
  - **Baseline RAG Implementation:** Implement a baseline RAG system by integrating a generator LLM with the retriever.

- **Month 3: Verifier Module - Signal Implementation (Part 1)**
  - **Intrinsic Uncertainty Detector:** Implement modules to calculate uncertainty from the generator's output, focusing on token-level entropy and perplexity spikes.
  - **Retrieval-Grounded Heuristics:** Implement heuristics to score evidence quality, such as `evidence coverage` (entity/noun phrase overlap) and `citation span integrity`.

- **Month 4: Verifier Module - Signal Implementation (Part 2)**
  - **Zero-Shot NLI Contradiction Detector:** Integrate an off-the-shelf NLI model (e.g., `MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli`) to check for contradictions between the LLM's claim and the retrieved evidence.
  - **Self-Agreement Detector:** Implement a self-consistency check by sampling multiple generations and measuring claim variability.

- **Month 5: Detector Evaluation & Mitigation (Optional)**
  - **Rule-Based Aggregation:** Combine all detector signals using explicit rules or a weighted heuristic to produce a final confidence score.
  - **End-to-End Detector Evaluation:** Evaluate the complete detector's ability to identify hallucinations on benchmarks (TruthfulQA, RAGTruth).
  - **Performance Analysis:** Measure key metrics like detection accuracy, precision, and recall.
  - **(If time permits) Simple Mitigation:** Implement basic mitigation logic (e.g., flagging or suppressing low-confidence claims).

- **Month 6: Finalization & Documentation**
  - **Ablation Study:** Analyze the contribution of each **trainless signal** by systematically disabling them and measuring the impact on detection performance.
  - **Final Report & Demo:** Write the final project report, detailing the architecture, methodology, results, and insights. Prepare a compelling demo of the system.
  - **Code Cleanup & Handoff:** Clean, comment, and document the codebase for future use and handoff.

---

## 3. Data Requirements and Collection (Trainless Detector Focus)

For the initial trainless phase of the project, data serves three primary purposes: providing a knowledge base for grounding, evaluating the detector's performance, and generating operational data for analysis.

### Core Data Requirements:

1.  **Knowledge Corpus (for Grounding):**
    *   **Source:** A recent snapshot of the **English Wikipedia**.
    *   **Purpose:** This corpus acts as the single source of truth for the RAG system. All generated claims will be verified against the information contained within it.
    *   **Processing:** The text will be cleaned, chunked into sentence-level fragments, and indexed into a **FAISS vector database**. Each chunk will retain metadata (e.g., `page_id`, `sentence_span`) for precise citation.

2.  **Benchmark Datasets (for Evaluation & Threshold Calibration):**
    *   **Purpose:** These datasets are essential for measuring the effectiveness of the hallucination detector and for calibrating the thresholds of the rule-based aggregator.
    *   **Key Datasets:**
        *   **`TruthfulQA`:** To evaluate the detector's ability to identify plausible but false statements.
        *   **`RAGTruth`:** A highly relevant benchmark specifically designed to test for hallucinations in RAG-generated answers.
        *   **`FEVER`:** While a training dataset, its test set is invaluable for evaluating the Zero-Shot NLI component's ability to classify claims as "Supported" or "Refuted" against provided evidence.

3.  **Generated Operational Data (for Analysis):**
    *   **Purpose:** This data is generated by the system itself during operation and is crucial for analysis and ablation studies.
    *   **Components:**
        *   **Retrieved Evidence Embeddings:** The vector representations of all evidence chunks retrieved for a given query.
        *   **System Logs:** Detailed logs capturing the raw outputs of each detector signal (e.g., entropy scores, NLI probabilities, coverage metrics) for every claim. This data is vital for debugging and for the final ablation study.

### Data Collection Methods:
1.  **Knowledge Corpus:** Download and process a recent dump of the English Wikipedia.
2.  **Benchmark Datasets:** Download `TruthfulQA`, `RAGTruth`, and `FEVER` from official sources like Hugging Face Datasets or their respective GitHub repositories.
3.  **Operational Data:** This data will be generated and stored locally as the system runs evaluations.

---

### Optional Data for Future Trained Components

Should the project proceed to implement trained models for the verifier or mitigation modules, the following datasets will become relevant. They are not required for the initial trainless phase.

1.  **(Optional) Supervised Training Signals:**
    *   **Claim-Evidence-Label Triples:** Datasets like **FEVER/FEVEROUS** and **KILT-FEVER** would be used to train a custom evidence alignment scorer or a specialized NLI model.
    *   **General-Purpose NLI Datasets:** **SNLI, MNLI, and ANLI** would be used for fine-tuning a model to better understand entailment and contradiction.

2.  **(Optional) Retrieval/Reranking Datasets:**
    *   Subsets from the **BEIR** benchmark (e.g., **NQ, HotpotQA**) or the **MS MARCO Passage Ranking** dataset would be used to train an improved retriever or a dedicated reranker model.

3.  **(Optional) Custom Evaluation Set:**
    *   A **small-scale, custom-built "citation-annotated" evaluation set** would be created to provide a highly specialized testbed for the system's citation verification capabilities.


---

## 4. Supporting Research

The project builds upon established and recent research in the following areas:

### Overviews & Surveys
- **Paper:** "A Survey on Hallucination in Large Language Models" (Zhang et al., 2023)
- **Use:** Provides a comprehensive overview of the types of hallucinations, their causes, and the state-of-the-art detection and mitigation techniques, which helps to frame the project's contribution.
- **Link:** https://arxiv.org/abs/2311.05232

### Foundational Fact-Checking & Attribution
- **Paper:** "FEVER: a Large-scale Dataset for Fact Extraction and VERification" (Thorne et al., 2018)
- **Use:** Pioneered the claim-evidence-label framework ("Supported/Refuted/NotEnoughInfo"), establishing the methodology for claim-by-claim verification.
- **Link:** https://arxiv.org/abs/1803.05355

- **Paper:** "KILT: a Benchmark for Knowledge Intensive Language Tasks" (Petroni et al., 2021)
- **Use:** Created a benchmark for knowledge-intensive tasks with source attribution, emphasizing traceability to the page or paragraph level.
- **Link:** https://arxiv.org/abs/2009.02252

### Automated Factuality Evaluation
- **Paper:** "QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization" (Fabbri et al., 2021)
- **Use:** Provides a question-answering based method to evaluate factual consistency, which can be adapted to verify claims against evidence by generating questions from the claim and checking answers from the source document.
- **Link:** https://arxiv.org/abs/2112.08542

- **Paper:** "Evaluating the Factual Consistency of Abstractive Text Summarization" (Kryscinski et al., 2019)
- **Use:** Introduces a model to classify claim-evidence pairs as factually consistent or inconsistent, providing a direct blueprint for a component of our verifier module.
- **Link:** https://arxiv.org/abs/1910.12840

- **Paper:** "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization" (Laban et al., 2022)
- **Use:** Supports the project's core idea of using a Natural Language Inference (NLI) model for fine-grained, sentence-level alignment between the claim and the evidence to detect contradictions.
- **Link:** https://arxiv.org/abs/2111.09525

### Hallucination Benchmarking & Analysis
- **Paper:** "TruthfulQA: Measuring How Models Mimic Human Falsehoods" (Lin et al., 2021)
- **Use:** Demonstrated that models often generate plausible but false answers, highlighting the need for uncertainty estimation and verification.
- **Link:** https://arxiv.org/abs/2109.07958

- **Paper:** "RAGTruth: A Hallucination Benchmark for Retrieval-Augmented Generation" (2023)
- **Use:** A benchmark focused on hallucinations in RAG systems, showing that "cited hallucinations" are common even with retrieved evidence, which necessitates finer-grained evidence alignment and discrimination.
- **Link:** https://arxiv.org/abs/2401.00396

### Hallucination Detection & Mitigation Techniques
- **Paper:** "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models" (Manakul et al., 2023)
- **Use:** Uses self-consistency sampling to detect unreliable statements. This can serve as a useful uncertainty signal but is insufficient on its own to guarantee citation correctness.
- **Link:** https://arxiv.org/abs/2303.08896

- **Paper:** "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection" (Asai et al., 2023)
- **Use:** A model that reflects on its own generations and retrieves evidence for self-verification during the generation process, improving attribution and reducing hallucinations.
- **Link:** https://arxiv.org/abs/2310.11511

- **Paper:** "Chain-of-Verification Reduces Hallucination in Large Language Models" (Gao et al., 2023/2024)
- **Use:** An "answer-then-verify" process that automatically checks and rewrites its initial response, significantly improving the factuality of long-form answers.
- **Link:** https://arxiv.org/abs/2309.11495

### Critiques of LLM-as-a-Judge
- **Paper:** "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena" (Zheng et al., 2023)
- **Use:** Highlights the challenges and biases (e.g., position, verbosity, and self-preference biases) of using LLMs as evaluators. This supports the project's decision to use a more robust, multi-component verification system instead of relying solely on an LLM judge.
- **Link:** https://arxiv.org/abs/2306.05685

**Key Insight:** Research also indicates that using an LLM-as-a-judge can be prone to bias and instability, making it risky to rely solely on it for verifying citations. A more robust approach, which this project adopts, is to combine evidence retrieval with NLI-based contradiction detection.
