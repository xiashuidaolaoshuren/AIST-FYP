### Paper Summary: FEVER: a large-scale dataset for Fact Extraction and VERification

**1. Core Research Question (PICO/T):**
- **P (Problem):** There was a lack of large-scale, publicly available datasets for the task of verifying factual claims against textual evidence, which hindered progress in automated fact-checking.
- **I (Intervention):** The authors introduced **FEVER (Fact Extraction and VERification)**, a new dataset of 185,445 claims created by altering sentences from Wikipedia.
- **C (Comparison):** The paper's baseline model is compared against "oracle" models (e.g., one given the perfect evidence sentence) to demonstrate the difficulty of the individual sub-tasks (retrieval and classification). The dataset itself is implicitly compared to prior, smaller fact-checking datasets.
- **O (Outcome):** The primary outcome was the creation of a challenging benchmark. The authors measured the performance of a baseline system, which achieved only 31.87% accuracy on the full task, confirming its difficulty and value to the research community.
- **T (Theoretical Hypothesis):** The authors argued that a large-scale, challenging dataset is essential to stimulate progress in claim verification. They posited that by breaking the task into evidence retrieval and claim classification, and by providing a benchmark to measure performance on both, the community could develop more sophisticated and robust fact-checking systems.

**2. Methodology:**
The research methodology involved **dataset creation and benchmarking**:
1.  **Claim Generation:** Claims were created by taking sentences from Wikipedia and applying rule-based mutations (e.g., swapping entities, changing dates) to generate claims that could be supported or refuted by the original text. Claims that could not be verified were also included.
2.  **Human Annotation:** Human annotators, without seeing the original sentence, classified each claim as "Supported," "Refuted," or "NotEnoughInfo." For the first two labels, annotators were required to find and cite the specific sentence(s) from Wikipedia that constituted the evidence.
3.  **Baseline System Development:** A three-stage pipeline was built as a baseline: (1) a document retrieval component based on TF-IDF to find relevant Wikipedia pages, (2) a sentence selection component to find evidence within those pages, and (3) a textual entailment model to classify the claim based on the selected evidence.

**3. Key Findings:**
- **A Challenging Dataset:** The primary finding was that the created dataset was very difficult for contemporary models. The baseline system's low accuracy (31.87%) highlighted the gap between machine and human performance.
- **Information Retrieval is a Bottleneck:** The oracle experiments showed that even if a model was given the perfect evidence, classification was still non-trivial. However, the gap between the full pipeline and the oracle demonstrated that finding the correct evidence was a major part of the challenge.
- **Robust Annotation:** The annotation process was reliable, achieving a Fleiss' Îº score of 0.6841, which indicates substantial agreement among human annotators and validates the quality of the dataset labels.
- **Decomposability of the Task:** The paper successfully framed fact verification as a two-stage problem: evidence retrieval followed by textual entailment (or Natural Language Inference).

**4. Main Contribution:**
The main contribution is the **FEVER dataset itself**. It was a landmark resource that standardized the task of fact verification against a large textual corpus. It filled a critical gap by providing a large-scale training and evaluation benchmark, which catalyzed a wave of new research and models in the NLP community focused on fact-checking and evidence-based reasoning.

**5. Limitations:**
- **Synthetic Claims:** The claims were generated via rule-based alterations of Wikipedia sentences. This process may not capture the full diversity and subtlety of misinformation found in the real world, which is often more complex than simple entity swaps.
- **Single-Hop Reasoning:** The vast majority of claims in FEVER can be verified using a single sentence or a small set of sentences from one document. The dataset is not well-suited for evaluating complex, multi-hop reasoning that requires synthesizing information across multiple documents.
- **Wikipedia-Only:** The entire dataset is based on Wikipedia as the sole knowledge source. This limits its applicability to scenarios involving more diverse, noisy, and less-structured text from the open web.

**6. Keywords:**
- Fact Verification
- Dataset
- Textual Entailment
- Information Retrieval
- Claim Verification
- Wikipedia

**7. Relevance Assessment:**
- **Relevance:** High
- **Reason:** The FEVER dataset is foundational to this project. The project's core task of verifying a claim against evidence and classifying it as supported or refuted is precisely the task defined by FEVER. The "Supported/Refuted/NotEnoughInfo" classification scheme and the requirement to cite evidence are directly adopted in the project's design. FEVER will serve as a key training dataset for the project's evidence alignment scorer and NLI contradiction detector.