# Summary of "FEVER: a Large-scale Dataset for Fact Extraction and VERification" (Thorne et al., 2018)

## 1. Core Research Question (PICO/T)

- **P (Problem/Population):** Progress in automated fact-checking is hampered by the lack of large-scale, high-quality datasets. Existing resources are either too small (a few hundred claims) or do not require systems to retrieve evidence from a large corpus, unlike real-world verification scenarios.
- **I (Intervention/Interest):** The paper introduces FEVER (Fact Extraction and VERification), a new, large-scale dataset of 185,445 claims. These claims were generated by humans who altered sentences from Wikipedia. A separate group of annotators then classified each claim as SUPPORTED, REFUTED, or NOT ENOUGH INFO, and crucially, for the first two classes, they had to retrieve the specific sentence(s) from Wikipedia that constituted the evidence.
- **C (Comparison):** The authors develop a baseline pipeline system to characterize the dataset's difficulty. This pipeline consists of three stages: document retrieval, sentence-level evidence selection, and textual entailment classification. They also use oracle experiments (where parts of the pipeline are replaced with gold-standard data) to identify the most challenging sub-task.
- **O (Outcome):** The primary outcome is the FEVER dataset itself. The baseline system achieves an accuracy of 50.91% when only classifying the claim, but this drops to just 31.87% when the system is also required to provide the correct evidence. The oracle experiments reveal that sentence-level evidence selection is the biggest bottleneck.
- **T (Theory):** The central argument is that a large-scale dataset that requires both evidence retrieval and claim classification is essential for stimulating progress in claim verification. The low baseline scores demonstrate that FEVER is a challenging testbed that will drive the development of more sophisticated models that can jointly reason over retrieved evidence.

## 2. Methodology

The study's methodology involved a two-stage data creation process followed by a baseline evaluation:
1.  **Claim Generation:** Annotators were given sentences from popular Wikipedia pages and asked to generate claims by extracting and mutating information. Mutations included paraphrasing, negation, and substituting entities, designed to create a mix of true, false, and unverifiable statements.
2.  **Claim Labeling & Evidence Retrieval:** A separate set of annotators, who did not know the original source sentence, were tasked with labeling each claim. For claims labeled SUPPORTED or REFUTED, they had to search through Wikipedia to find and select the minimal set of sentences that justified their verdict.
3.  **Data Validation:** Rigorous validation was performed, including 5-way inter-annotator agreement (achieving a Fleiss κ of 0.6841), comparison against "super-annotators" to measure evidence recall, and manual validation by the authors.
4.  **Baseline System:** A three-part pipeline was built:
    *   **Document Retrieval:** A standard TF-IDF based method to find relevant Wikipedia pages.
    *   **Sentence Selection:** A TF-IDF based method to rank and select sentences from the retrieved documents.
    *   **Recognizing Textual Entailment (RTE):** A Decomposable Attention model to classify the claim as SUPPORTED, REFUTED, or NOT ENOUGH INFO based on the selected evidence.

## 3. Key Findings

- **Dataset Scale and Quality:** The FEVER dataset is orders of magnitude larger than previous fact-checking datasets, providing a robust resource for training and evaluating complex models.
- **Evidence Retrieval is the Hardest Part:** The baseline system's accuracy plummeted from 50.91% to 31.87% when forced to provide correct evidence. Manual error analysis confirmed that the information retrieval component failing to find the right evidence was the largest source of error (58.27% of incorrect predictions).
- **Multi-hop Reasoning is Required:** A significant portion of claims requires evidence from multiple sentences (16.82%) and sometimes even multiple Wikipedia pages (12.15%), pushing models beyond simple single-sentence verification.
- **Simulating "Not Enough Info" is Important:** The strategy for training the NOT ENOUGH INFO class was critical. Using sentences from pages related to the claim (but not directly useful) as negative examples proved more effective than using completely random sentences.
- **Annotation is Challenging but Feasible:** The study demonstrates a successful methodology for creating a large-scale verification dataset, achieving respectable inter-annotator agreement on a complex task that combines search, retrieval, and inference.

## 4. Main Contribution

The primary contribution is the **FEVER dataset** itself. It was the first large-scale benchmark to model fact-checking as a multi-stage task requiring both the retrieval of evidence from a large text corpus (Wikipedia) and the subsequent classification of a claim based on that evidence. This shifted the paradigm from simple textual entailment (where evidence is given) to a more realistic verification scenario, paving the way for the development of modern retrieval-augmented models.

## 5. Limitations

- **Source is Limited to Wikipedia:** The dataset is entirely based on Wikipedia, which, while comprehensive, does not represent the diversity of text styles and sources found on the open web.
- **Focus on Introductory Sections:** The initial claim generation and evidence search were largely focused on the introductory sections of Wikipedia pages, which may not capture the full complexity of the entire article.
- **Potential for Annotation Gaps:** While the annotation process was rigorous, it's possible that annotators missed some valid evidence sentences, which could unfairly penalize a system that finds an alternative, correct justification.
- **Static Knowledge:** The dataset is based on a 2017 snapshot of Wikipedia and does not address the challenge of verifying claims against information that changes over time.

## 6. Keywords

- Fact Verification
- Dataset
- Evidence Retrieval
- Textual Entailment
- Wikipedia

## 7. Relevance Assessment

- **Relevance:** High
- **Justification:** This paper is foundational to my project. It introduces the **FEVER dataset**, which is one of the primary datasets I plan to use for training and evaluating my verifier module. The paper's core task—classifying a claim as SUPPORTED or REFUTED based on retrieved evidence—is exactly what my system is designed to do. The baseline results and error analysis strongly justify my project's focus on improving the evidence-to-claim verification step, which this paper identifies as the main bottleneck.
