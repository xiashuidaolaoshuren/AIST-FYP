Supervisor Topics: LLM Project Proposals (Feasible for 2-Person, 6 Months, RTX 3070Ti)
====================================================================================
Date: September 11, 2025
Team Size: 2 | Duration: Max 6 months | GPU: RTX 3070Ti (optional 3090)

Overview
--------
This document proposes three focused, innovative, and feasible LLM projects suggested by the supervisor: (1) Hallucination Detection & Mitigation, (2) LLM-as-a-Judge with Bias Control, (3) Data Contamination Detection & Prevention. Each section specifies goals, novelty, datasets, methods, evaluation, timeline, risks, and deliverables tuned for a 2-person team and RTX 3070Ti.

===============================================================================
1) Hallucination Detection & Mitigation for LLMs
-----------------------------------------------
Goal
- Detect factual hallucinations in LLM outputs and reduce them via lightweight mitigation (RAG filters, verifiers, or editing), targeting general QA and domain QA (e.g., academic or product KB).

What’s new (research gap)
- Lightweight verifier ensemble that combines retrieval evidence scoring + contradiction detection + uncertainty signals (self-consistency, logprob, entropy) without needing a huge judge model.
- Efficient training with QLoRA/LoRA on RTX 3070Ti for a small verifier (<=7B) and selective RAG fallback; plug-and-play with any base LLM.

Data & Benchmarks
- TruthfulQA, FEVER, RAGTruth, HaluEval/HaluBench, HotpotQA (evidence-based), PubMedQA (domain), NQ-open.
- Evidence corpora: Wikipedia dump or domain-specific KBs; use BM25 + dense retriever (e.g., bge-small) for top-k passages.
- Optional: Synthetic hard negatives via prompt-based contradiction generation.

Method (modular pipeline)
- Generator: a compact open LLM (e.g., Llama-2/3 7B, Mistral 7B, Qwen2 7B) with instruction-tuning for QA.
- Retrieval: hybrid retriever (BM25 + small embedding model). Evidence pack into verifier.
- Verifier ensemble (key novelty):
  1) Evidence alignment scorer (is each claim supported/contradicted/missing).
  2) Uncertainty signals (logprob, entropy, self-consistency across n samples).
  3) Contradiction/NLI head (tiny DeBERTa or RoBERTa NLI finetuned on claim-evidence pairs).
- Mitigation: if unsupported/contradicted or low confidence → trigger RAG rewrite or cite-only mode; optionally extract citations.

Training (RTX 3070Ti ready)
- Fine-tune small verifier models via LoRA/QLoRA (bf16/fp16, grad checkpointing, batch 2–8, GA for effective batch 32–128).
- Train NLI head on FEVER-style data (claim/evidence, 3-way labels). Train support scorer on evidence-labeled QA data.
- No full fine-tuning for the generator needed; use instruction-tuned checkpoints + prompt engineering.

Evaluation
- Hallucination rate (precision/recall), FEVER score, TruthfulQA accuracy, citation support rate, human eval on a 200-sample slice.
- Latency and throughput on RTX 3070Ti; ablation of each verifier signal; robustness under noisy retrieval.

6-month plan (2 people)
- M1: Literature review, data setup, retrieval index; baseline QA + naive RAG.
- M2: Implement uncertainty signals + simple evidence alignment; baseline metrics.
- M3: Train NLI/claim-evidence verifier; integrate ensemble; add citation extraction.
- M4: Mitigation strategies (rewrite/cite-only), error analysis, hard negative mining.
- M5: Optimization (quantization, batching), user study, write paper draft.
- M6: Final evaluation, docs, demo (web UI), thesis/report.

Team split
- Member A: Retrieval + verifier training + metrics.
- Member B: Mitigation policies + UI/API + experiments + documentation.

Risks & Mitigations
- Retrieval misses evidence → hybrid retriever + larger corpus + reranking.
- Verifier over-penalizes correct answers → calibration + threshold tuning + ensemble.

Deliverables
- Verifier model weights, pipeline repo, report with ablations, demo with citation toggle.

===============================================================================
2) LLM-as-a-Judge with Bias Control & Reliability Guarantees
------------------------------------------------------------
Goal
- Build a lightweight, calibrated LLM-based evaluator ("judge") that scores model outputs (e.g., helpfulness, factuality, safety) with bias control and confidence estimates, suitable for small labs.

What’s new (research gap)
- Bias-aware prompting and aggregation: judge with role-randomization, rubric shuffling, and reference-free vs reference-based dual-mode.
- Calibrated scoring: conformal prediction or isotonic calibration to attach confidence/uncertainty to scores.

Data & Benchmarks
- MT-Bench, LMSYS-Arena-Hard, Anthropic Helpful/Harmless comparisons, SummEval (for summarization), factual QA sets (FEVER, TruthfulQA) as judge targets.
- Pairwise comparison data (win/lose/tie) and absolute scoring rubrics (Likert 1–7).

Method
- Judge model: small instruct model (7B) or distilled critic head; optional preference model (DPO/SFT) trained on pairwise data.
- Bias reduction: multiple independent chains with rubric randomization and role shuffling; approval voting/majority consensus.
- Confidence: bootstrap over chains; conformal calibration map from variance → confidence intervals.
- Output: score + rationale + confidence + bias flags (e.g., verbosity bias, position bias).

Training (RTX 3070Ti ready)
- Distill judge from stronger references via LoRA/DPO on pairwise preference datasets.
- Use short contexts and limited max tokens; batch 2–8 with GA. Quantize for inference.

Evaluation
- Correlation with human judgments (Spearman/Kendall). Agreement rate with gold pairwise outcomes.
- Bias probes: position bias tests, verbosity bias tests, rubric sensitivity tests.
- Cost/latency vs baseline judges.

6-month plan (2 people)
- M1: Collect judge datasets, define rubrics, build evaluation harness.
- M2: Implement baseline judge prompts; run multi-chain aggregation; measure agreement.
- M3: Train LoRA/DPO critic head; add calibration; bias probes.
- M4: Domain extensions (code, SQL, safety); dashboards.
- M5: Ablations, documentation, write-up.
- M6: Final benchmark suite + public demo.

Team split
- Member A: Data + training (LoRA/DPO) + calibration.
- Member B: Prompting protocols + bias probes + tooling/UI.

Risks & Mitigations
- Overfitting to a benchmark → hold-out tasks + cross-domain testing.
- Residual bias → ensemble with rule-based checks; rubric audits.

Deliverables
- Judge model/checkpoints, evaluation harness, bias probe suite, technical report.

===============================================================================
3) Data Contamination Detection & Prevention in LLM Training
-----------------------------------------------------------
Goal
- Detect whether evaluation data leaked into training (contamination) and build a practical prevention/checklist pipeline for small labs.

What’s new (research gap)
- Practical, low-compute contamination auditing toolkit combining: canary exposure tests, n-gram/semantic overlap, PPL differentials, and membership inference approximations for LLMs.
- Prevention checklist integrating dataset versioning, URL logging, and dedup filters; automated CI guardrails.

Data & Benchmarks
- Public corpora snapshots (Common Crawl, The Pile subsets) with timestamps; standardized eval sets (MMLU, GSM8K, TruthfulQA, HumanEval, MBPP).
- Synthetic canary datasets and date-stamped web samples for regression tests.

Method
- Overlap detectors: minhash/SimHash for n-gram overlap; embedding-based nearest neighbor for semantic duplicates.
- Perplexity differential: compare PPL under base vs fine-tuned model—suspicious drops indicate leak.
- Canary exposure: insert known rare strings during training; measure exposure under prompting.
- Membership inference (approx.): confidence/logprob gap tests for seen vs unseen.
- CI workflow: enforce split-by-date, dedup, URL provenance logging, automated reports.

Training (RTX 3070Ti ready)
- Mostly analysis; small models for probes (e.g., 1–3B or 7B for PPL). Use quantized checkpoints. Batch small; process data in shards.

Evaluation
- Precision/recall of contamination flags on synthetic leaks; case studies on known leaked vs clean splits.
- Runtime and cost for small labs; false positive analysis.

6-month plan (2 people)
- M1: Implement overlap + URL provenance logging; baseline audits on sample corpora.
- M2: Add PPL differential + canary tests; synthetic leak experiments.
- M3: Add semantic duplicate detection; membership inference.
- M4: Build CI/CLI tool + report generator.
- M5: Case studies, robustness tests, documentation.
- M6: Release toolkit + paper-style report.

Team split
- Member A: Overlap/inference algorithms + probes.
- Member B: Data pipeline + CI tooling + reporting UI.

Risks & Mitigations
- High false positives → multi-signal fusion + thresholds + manual audit UI.
- Large data cost → sampled audits + streaming processing.

Deliverables
- Auditing toolkit (CLI/Lib), contamination reports, reproducible case studies, best-practice checklist.

===============================================================================
Feasibility & Compute Plan (3070Ti / 3090)
-----------------------------------------
- Use small models (7B) for training with LoRA/QLoRA; bf16/fp16 mixed precision; gradient accumulation; gradient checkpointing.
- Quantized inference (4/8-bit) for verifiers/judges; batch small contexts.
- For heavier experiments (13B+), borrow 3090 or brief cloud sessions.

Common Tooling for All Three Projects
-------------------------------------
- Frameworks: PyTorch, Hugging Face Transformers + PEFT (LoRA/QLoRA), bitsandbytes, datasets.
- Retrieval/Embeddings: FAISS, BM25 (Lucene), bge-small/e5-small.
- Experiment tracking: Weights & Biases or TensorBoard; Hydra configs.
- Serving: FastAPI/Gradio; ONNX/TensorRT optional for small encoders.
- Evaluation harnesses and unit tests; dataset versioning with DVC or Git LFS.

Success Criteria (per project)
-----------------------------
- Hallucination: ≥X% reduction in hallucination rate at similar helpfulness; strong FEVER/TruthfulQA gains.
- LLM-as-a-Judge: High correlation with human ratings; reduced bias (position/verbosity) with confidence calibration.
- Contamination: Accurate detection on synthetic leaks; actionable CI tool with low runtime for small labs.

Stretch Goals (optional)
------------------------
- Hallucination: citation-grounded generation; selective abstention with reasons.
- Judge: task-adaptive rubrics; multi-judge ensembles; cost-optimal routing.
- Contamination: web-scale sampling strategy; privacy-preserving membership inference.
